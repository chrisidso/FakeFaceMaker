{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "import skimage.filters\n",
    "import skimage.feature\n",
    "import skimage.restoration\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import sobel\n",
    "from skimage.feature import canny\n",
    "from skimage.restoration import denoise_bilateral, denoise_tv_chambolle\n",
    "from skimage.io import imread, imshow\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import misc\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread, imshow\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "sys.path.insert(0,'/home/newton/images')\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.experimental\n",
    "import keras.applications.mobilenet\n",
    "from keras.preprocessing import image\n",
    "import file_image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir= '/home/newton/images/gender_images'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "batch_size = 100\n",
    "IMG_W = 224\n",
    "IMG_H = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_train = ImageDataGenerator(rescale=1/255)\n",
    "train_data_gen = image_gen_train.flow_from_directory(\n",
    "                                                batch_size=batch_size, \n",
    "                                                directory=train_dir, \n",
    "                                                shuffle=True, \n",
    "                                                target_size=(IMG_W,IMG_H),\n",
    "                                                class_mode='sparse'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_val = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size, \n",
    "                                                 directory=val_dir, \n",
    "                                                 target_size=(IMG_W, IMG_H),\n",
    "                                                 class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n",
    "\n",
    "feature_extractor = hub.KerasLayer(URL)\n",
    "\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "m1_gender = tf.keras.Sequential([\n",
    "  feature_extractor,\n",
    "  layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_gen[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_gender.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "80/80 [==============================] - 278s 3s/step - loss: 0.1659 - acc: 0.9352 - val_loss: 0.2009 - val_acc: 0.9270\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 277s 3s/step - loss: 0.1581 - acc: 0.9381 - val_loss: 0.2002 - val_acc: 0.9225\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 278s 3s/step - loss: 0.1574 - acc: 0.9374 - val_loss: 0.2110 - val_acc: 0.9165\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 276s 3s/step - loss: 0.1524 - acc: 0.9411 - val_loss: 0.1977 - val_acc: 0.9280\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - 277s 3s/step - loss: 0.1466 - acc: 0.9438 - val_loss: 0.1956 - val_acc: 0.9260\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "history = m1_gender.fit(\n",
    "    train_data_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     multiple                  2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1_gender.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_gender.save('./models/gender_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('./models/gender_model_1.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     multiple                  2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image used here is female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = prepare_image(os.path.join('/home/newton/images/gender_images/train/female','008642.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_model.predict(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9848807e-02 9.0014851e-01 2.1657875e-07 4.4057771e-07 2.0590005e-06]]\n"
     ]
    }
   ],
   "source": [
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001485"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
