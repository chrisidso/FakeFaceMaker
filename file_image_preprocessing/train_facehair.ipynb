{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform\n",
    "import skimage.filters\n",
    "import skimage.feature\n",
    "import skimage.restoration\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import sobel\n",
    "from skimage.feature import canny\n",
    "from skimage.restoration import denoise_bilateral, denoise_tv_chambolle\n",
    "from skimage.io import imread, imshow\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import misc\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread, imshow\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "sys.path.insert(0,'/home/newton/images')\n",
    "import os\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import tensorflow.keras.experimental\n",
    "import keras.applications.mobilenet\n",
    "from keras.preprocessing import image\n",
    "import file_image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir= '/home/newton/images/facehair_train'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "batch_size = 100\n",
    "IMG_W = 224\n",
    "IMG_H = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_train = ImageDataGenerator(rescale=1/255)\n",
    "train_data_gen = image_gen_train.flow_from_directory(\n",
    "                                                batch_size=batch_size, \n",
    "                                                directory=train_dir, \n",
    "                                                shuffle=True, \n",
    "                                                target_size=(IMG_W,IMG_H),\n",
    "                                                class_mode='sparse'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen_val = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size, \n",
    "                                                 directory=val_dir, \n",
    "                                                 target_size=(IMG_W, IMG_H),\n",
    "                                                 class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2\"\n",
    "\n",
    "feature_extractor = hub.KerasLayer(URL)\n",
    "\n",
    "feature_extractor.trainable = False\n",
    "\n",
    "m3_facehair = tf.keras.Sequential([\n",
    "  feature_extractor,\n",
    "  layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_facehair.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "64/64 [==============================] - 222s 3s/step - loss: 0.5165 - acc: 0.7456 - val_loss: 0.5538 - val_acc: 0.7181\n",
      "Epoch 2/15\n",
      "64/64 [==============================] - 221s 3s/step - loss: 0.5076 - acc: 0.7522 - val_loss: 0.5506 - val_acc: 0.7131\n",
      "Epoch 3/15\n",
      "64/64 [==============================] - 220s 3s/step - loss: 0.4993 - acc: 0.7545 - val_loss: 0.5482 - val_acc: 0.7244\n",
      "Epoch 4/15\n",
      "64/64 [==============================] - 221s 3s/step - loss: 0.4939 - acc: 0.7619 - val_loss: 0.5568 - val_acc: 0.7156\n",
      "Epoch 5/15\n",
      "64/64 [==============================] - 220s 3s/step - loss: 0.4880 - acc: 0.7636 - val_loss: 0.5533 - val_acc: 0.7250\n",
      "Epoch 6/15\n",
      "64/64 [==============================] - 220s 3s/step - loss: 0.4812 - acc: 0.7713 - val_loss: 0.5487 - val_acc: 0.7188\n",
      "Epoch 7/15\n",
      "64/64 [==============================] - 220s 3s/step - loss: 0.4794 - acc: 0.7731 - val_loss: 0.5487 - val_acc: 0.7237\n",
      "Epoch 8/15\n",
      "64/64 [==============================] - 219s 3s/step - loss: 0.4822 - acc: 0.7686 - val_loss: 0.5703 - val_acc: 0.7119\n",
      "Epoch 9/15\n",
      "64/64 [==============================] - 219s 3s/step - loss: 0.4714 - acc: 0.7766 - val_loss: 0.5474 - val_acc: 0.7262\n",
      "Epoch 10/15\n",
      "64/64 [==============================] - 221s 3s/step - loss: 0.4689 - acc: 0.7777 - val_loss: 0.5487 - val_acc: 0.7256\n",
      "Epoch 11/15\n",
      "64/64 [==============================] - 220s 3s/step - loss: 0.4649 - acc: 0.7763 - val_loss: 0.5492 - val_acc: 0.7275\n",
      "Epoch 12/15\n",
      "64/64 [==============================] - 221s 3s/step - loss: 0.4613 - acc: 0.7803 - val_loss: 0.5476 - val_acc: 0.7237\n",
      "Epoch 13/15\n",
      "64/64 [==============================] - 221s 3s/step - loss: 0.4631 - acc: 0.7819 - val_loss: 0.5780 - val_acc: 0.7150\n",
      "Epoch 14/15\n",
      "64/64 [==============================] - 220s 3s/step - loss: 0.4568 - acc: 0.7839 - val_loss: 0.5553 - val_acc: 0.7225\n",
      "Epoch 15/15\n",
      "64/64 [==============================] - 221s 3s/step - loss: 0.4527 - acc: 0.7841 - val_loss: 0.5489 - val_acc: 0.7269\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "history = m3_facehair.fit(\n",
    "    train_data_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     multiple                  2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m3_facehair.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_facehair.save('./models/facehair_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('./models/facehair_model_1.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     multiple                  2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = prepare_image(os.path.join('/home/newton/images/gender_images/train/female','000001.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_model.predict(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the results seem to be that the first number is glasses, the second is no glasses.\n",
    "#  The other three are irrelevant.\n",
    "#  the image used here has glasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.8240178e-01 3.1759244e-01 3.4340760e-06 1.8126588e-06 5.8129763e-07]]\n"
     ]
    }
   ],
   "source": [
    "print (result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
